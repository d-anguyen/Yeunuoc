{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num GPUs 0\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from include import *\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import pywt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "GPU = True\n",
    "if GPU == True:\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    print(\"num GPUs\",torch.cuda.device_count())\n",
    "    device = 'cuda'\n",
    "    if torch.cuda.device_count()==0:\n",
    "        dtype = torch.FloatTensor\n",
    "        device = 'cpu'\n",
    "else:\n",
    "    dtype = torch.FloatTensor\n",
    "    device = 'cpu'\n",
    "\n",
    "import multiprocessing\n",
    "pool= multiprocessing.Pool((multiprocessing.cpu_count() -20))\n",
    "\n",
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load a test image from a dataset (now : CelebA 128x128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = 'mnist' # 'mnist' or 'celeba'\n",
    "dataset = 'celeba'\n",
    "path = './test_data/' + dataset + '/' \n",
    "img_name = dataset + '2' # 1-5 (for celeba), 1-6 (for mnist)\n",
    "img_path = path + img_name + \".jpg\"\n",
    "img_pil = Image.open(img_path)\n",
    "if dataset == 'celeba':\n",
    "    #img_pil = img_pil.crop((60,80+20,60+64,80+84)) #crop to 3 x 64 x 64\n",
    "    cx=89\n",
    "    cy=121\n",
    "    img_pil = img_pil.crop((cx-64, cy - 64, cx + 64, cy+64))\n",
    "\n",
    "img_np = pil_to_np(img_pil)\n",
    "print('Dimensions of input image:', img_np.shape)\n",
    "img_np = img_np / np.max(img_np)\n",
    "\n",
    "\n",
    "img_np_orig = 1*img_np\n",
    "\n",
    "if dataset == 'celeba':\n",
    "    plt.imshow(img_np.transpose(1,2,0))\n",
    "else:\n",
    "    plt.imshow(img_np[0,:,:])\n",
    "    plt.gray()\n",
    "plt.axis('off')\n",
    "\n",
    "save_path= './low_rate/Original'+'_'+img_name+'.png'\n",
    "plt.savefig(save_path, bbox_inches='tight', pad_inches = 0)\n",
    "\n",
    "img_var = np_to_var(img_np).type(dtype)\n",
    "d = img_np.shape[1]\n",
    "out_ch = img_np.shape[0]\n",
    "d_image = img_np.size\n",
    "\n",
    "# normalize the pixels to [-1,1]\n",
    "img_var = 2*img_var -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 0.1 #compression rate\n",
    "print('Compression rate is ', f)\n",
    "m_image = int(f*d_image)\n",
    "print('Number of measurements is ',m_image, ' for signal of length ', d_image)\n",
    "\n",
    "# random Gaussian measurement process\n",
    "\n",
    "A = torch.randn(m_image, d_image).to(device)\n",
    "x = img_var.to(device).reshape(d_image)\n",
    "y = torch.matmul(A,x).to(device)\n",
    "\n",
    "#latentDim = model.config.noiseVectorDim\n",
    "print(A.shape, x.shape, y.shape)\n",
    "mse = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Compressed sensing using generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Load a pretrained generative model on the dataset (now: PGGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True if torch.cuda.is_available() else False\n",
    "\n",
    "# trained on high-quality celebrity faces \"celebA\" dataset\n",
    "# this model outputs 512 x 512 pixel images\n",
    "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub',\n",
    "                       'PGAN', model_name='celeba',\n",
    "                       pretrained=True, useGPU=use_gpu)\n",
    "# this model outputs 256 x 256 pixel images\n",
    "# model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub',\n",
    "#                        'PGAN', model_name='celebAHQ-256',\n",
    "#                        pretrained=True, useGPU=use_gpu)\n",
    "G = model.netG\n",
    "#G.eval()\n",
    "latentDim = model.config.noiseVectorDim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.min(img_var), torch.max(img_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. CS using the loaded GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0= time.time()\n",
    "\n",
    "z0, mse_wrt_loss = CSGM2(G, latentDim, y, A, device, num_iter = 1600)\n",
    "x0 = G(z0)\n",
    "\n",
    "grid = torchvision.utils.make_grid(x0.clamp(min=-1, max=1), scale_each=True, normalize=True)\n",
    "plt.axis('off')\n",
    "plt.imshow(grid.detach().permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "save_path= './low_rate/PGGAN'+'_'+img_name+'.png'\n",
    "plt.savefig(save_path,bbox_inches='tight', pad_inches = 0) \n",
    "    \n",
    "t1= time.time()\n",
    "print('\\nTime elapsed:',t1-t0)\n",
    "\n",
    "error_wrt_truth = mse(x0, img_var).item()\n",
    "print('\\nl2-recovery error:', error_wrt_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''plt.xlabel('optimizer iteration')\n",
    "plt.ylabel('recovery error')\n",
    "plt.semilogy(mse_wrt_truth)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compressed Sensing using Deep decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use decoder architecture or DC GAN architecture\n",
    "decodetype = 'upsample' # transposeconv / upsample\n",
    "\n",
    "num_channels = [120,40,20,15,10] \n",
    "\n",
    "output_depth = img_np.shape[0] # number of output channels\n",
    "net = autoencodernet(num_output_channels=output_depth,num_channels_up=num_channels,need_sigmoid=True, \n",
    "                        decodetype=decodetype\n",
    "                        ).type(dtype)\n",
    "\n",
    "print(\"number of parameters: \", num_param(net))\n",
    "if decodetype == 'upsample':\n",
    "    print(net.decoder)\n",
    "elif decodetype == 'transposeconv':\n",
    "    print(net.convdecoder)\n",
    "net_in = copy.deepcopy(net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. CS using untrained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "net, net_input, loss = CS_DD(net, num_channels, d_image, y=y, A=A, device= device, \n",
    "                             num_iter = 6000, lr_decay_epoch=2000)#12000-3000\n",
    "x_DD = net( net_input.type(dtype) )#.data.cpu().numpy()[0]\n",
    "\n",
    "t1 = time.time()\n",
    "grid = torchvision.utils.make_grid(x_DD, scale_each=True, normalize=True)\n",
    "plt.imshow(grid.detach().permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis('off')\n",
    "#plt.imshow(x_hat.transpose(1,2,0))\n",
    "#plt.show()\n",
    "\n",
    "print('\\n time elapsed:', t1-t0)\n",
    "\n",
    "error_wrt_truth = mse(x_DD, img_var).item()\n",
    "print('\\nl2-recovery error:', error_wrt_truth)\n",
    "\n",
    "save_path= './low_rate/DD'+'_'+img_name+'.png'\n",
    "plt.savefig(save_path,bbox_inches='tight', pad_inches = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compressed sensing using hybrid model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Define the untrained network used for hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use decoder architecture or DC GAN architecture\n",
    "decodetype = 'upsample' # transposeconv / upsample\n",
    "\n",
    "num_channels = [120,40,20,15,10] \n",
    "\n",
    "output_depth = img_np.shape[0] # number of output channels\n",
    "#net = autoencodernet(num_output_channels=output_depth,num_channels_up=num_channels,need_sigmoid=True, \n",
    "#                        decodetype=decodetype\n",
    "#                        ).type(dtype)\n",
    "\n",
    "print(\"number of parameters: \", num_param(net))\n",
    "if decodetype == 'upsample':\n",
    "    print(net.decoder)\n",
    "elif decodetype == 'transposeconv':\n",
    "    print(net.convdecoder)\n",
    "net_copy = copy.deepcopy(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. CS using hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "#initialization by csgm\n",
    "#z0, mse_wrt_truth = CSGM2(G=G, latentDim=latentDim, y=y, A=A, device=device, num_iter=600)\n",
    "#x0 = G(z0)\n",
    "\n",
    "grid = torchvision.utils.make_grid(x0.clamp(min=-1, max=1), scale_each=True, normalize=True)\n",
    "plt.axis('off')\n",
    "plt.imshow(grid.detach().permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "# performing optimization\n",
    "net, net_input, z, alpha, beta, loss = CS_hybrid2(G, net, net_input, num_channels, d_image, y, A, z_0 = z0, \n",
    "                                            latentDim=latentDim, num_iter = 1000, lr_decay_epoch = 0)\n",
    "\n",
    "x_hat = alpha.clamp(0,1)*G(z) + beta.clamp(0,1)*(2*net(net_input.type(dtype)) - 1)\n",
    "\n",
    "print(alpha,beta)\n",
    "\n",
    "grid = torchvision.utils.make_grid(x_hat, scale_each=True, normalize=True)\n",
    "plt.axis('off')\n",
    "plt.imshow(grid.detach().permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "t1 = time.time()\n",
    "print('\\n time elapsed:', t1-t0)\n",
    "\n",
    "error_wrt_truth = mse(x_hat, img_var)\n",
    "print('\\nl2-recovery error:', error_wrt_truth)\n",
    "\n",
    "\n",
    "save_path= './low_rate/Hybrid'+'_'+img_name+'.png'\n",
    "plt.savefig(save_path,bbox_inches='tight', pad_inches = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
